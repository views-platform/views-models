
def get_hp_config():
    """
    Contains the hyperparameter configurations for model training.
    This configuration is "operational" so modifying these settings will impact the model's behavior during the training.

    Returns:
    - hyperparameters (dict): A dictionary containing hyperparameters for training the model, which determine the model's behavior during the training phase.
    """
    
    hyperparameters = {
        "steps": [*range(1, 36 + 1, 1)],
        # Good! silvery-sweep-1
        # "batch_size": 64,
        # "decoder_output_dim": 8,
        # "delta": 3.696974087786076,
        # "dropout": 0.2,
        # "early_stopping_patience": 2,
        # "false_negative_weight": 5.0977656184083395,
        # "false_positive_weight": 4.507821247953932,
        # "feature_scaler": None,
        # "gradient_clip_val": 0.2,
        # "hidden_size": 256,
        # "input_chunk_length": 72,
        # "loss_function": "WeightedPenaltyHuberLoss",
        # "lr": 0.00009976652946422868,
        # "n_epochs": 5,
        # "non_zero_weight": 1.7773296987133582,
        # "num_decoder_layers": 4,
        # "num_encoder_layers": 1,
        # "target_scaler": None,
        # "temporal_decoder_hidden": 32,
        # "use_layer_norm": True,
        # "weight_decay": 0.00533531735917749,
        # "zero_threshold": 0.17066172076836716,

        #crimson-sweep-169. Psychotic.
        # "batch_size": 128,
        # "decoder_output_dim": 16,
        # "delta": 1.7234705394805443,
        # "dropout": 0.1,
        # "early_stopping_patience": 6,
        # "false_negative_weight": 9.3313776622516,
        # "false_positive_weight": 10.78492842081859,
        # "feature_scaler": "MaxAbsScaler",
        # "gradient_clip_val": 0.8,
        # "hidden_size": 128,
        # "input_chunk_length": 36,
        # "loss_function": "WeightedPenaltyHuberLoss",
        # "lr": 0.00001471988186818606,
        # "n_epochs": 300,
        # "non_zero_weight": 13.23535161231906,
        # "num_decoder_layers": 2,
        # "num_encoder_layers": 5,
        # "target_scaler": "MaxAbsScaler",
        # "temporal_decoder_hidden": 16,
        # "use_layer_norm": False,
        # "weight_decay": 0.0000214973309074918,
        # "zero_threshold": 0.26369001329592373,

        #peachy-sweep-64
    #     "batch_size": 32,
    #     "decoder_output_dim": 32,
    #     "delta": 3.402036667021411,
    #     "dropout": 0.4,
    #     # "early_stopping_patience": 6,
    #     "false_negative_weight": 0.4094813655878722,
    #     "false_positive_weight": 5.344567094760235,
    #     "feature_scaler": None,
    #     "gradient_clip_val": 0.2,
    #     "hidden_size": 32,
    #     "input_chunk_length": 48,
    #     "loss_function": "WeightedPenaltyHuberLoss",
    #     "lr": 0.0008657902172163073,
    #     "n_epochs": 300,
    #     "non_zero_weight": 13.833818931287857,
    #     "num_decoder_layers": 2,
    #     "num_encoder_layers": 2,
    #     "target_scaler": "MinMaxScaler",
    #     "temporal_decoder_hidden": 16,
    #     "use_layer_norm": False,
    #     "weight_decay": 0.00010196612489718342,
    #     "zero_threshold": 0.0965370826949934,


        "num_samples": 1,
        "mc_dropout": True,

        "activation": "SwiGLU",
        "batch_size": 16,
        "d_model": 128,
        "delta": 0.44655168450634775,
        "dim_feedforward": 512,
        "dropout": 0.1,
        "early_stopping_min_delta": 0.001,
        "early_stopping_patience": 20,
        "false_negative_weight": 4.71055731970463,
        "false_positive_weight": 2.1387284483229023,
        "feature_scaler": None,
        "feature_scaler_map": {
            "MinMaxScaler": [
                "lr_wdi_sl_tlf_totl_fe_zs",
                "lr_wdi_se_enr_prim_fm_zs",
                "lr_wdi_sp_urb_totl_in_zs",
                "lr_wdi_sh_sta_maln_zs",
                "lr_wdi_sh_sta_stnt_zs",
                "lr_wdi_dt_oda_odat_pc_zs",
                "lr_wdi_ms_mil_xpnd_gd_zs",
                "lr_vdem_v2x_horacc",
                "lr_vdem_v2xnp_client",
                "lr_vdem_v2x_veracc",
                "lr_vdem_v2x_divparctrl",
                "lr_vdem_v2xpe_exlpol",
                "lr_vdem_v2x_diagacc",
                "lr_vdem_v2xpe_exlgeo",
                "lr_vdem_v2xpe_exlgender",
                "lr_vdem_v2xpe_exlsocgr",
                "lr_vdem_v2x_ex_party",
                "lr_vdem_v2x_genpp",
                "lr_vdem_v2xeg_eqdr",
                "lr_vdem_v2xcl_prpty",
                "lr_vdem_v2xeg_eqprotec",
                "lr_vdem_v2x_ex_military",
                "lr_vdem_v2xcl_dmove",
                "lr_vdem_v2x_clphy",
                "lr_vdem_v2x_hosabort",
                "lr_vdem_v2xnp_regcorr",
                "lr_topic_ste_theta0",
                "lr_topic_ste_theta1",
                "lr_topic_ste_theta2",
                "lr_topic_ste_theta3",
                "lr_topic_ste_theta4",
                "lr_topic_ste_theta5",
                "lr_topic_ste_theta6",
                "lr_topic_ste_theta7",
                "lr_topic_ste_theta8",
                "lr_topic_ste_theta9",
                "lr_topic_ste_theta10",
                "lr_topic_ste_theta11",
                "lr_topic_ste_theta12",
                "lr_topic_ste_theta13",
                "lr_topic_ste_theta14",
                "lr_topic_ste_theta0_stock_t1_splag",
                "lr_topic_ste_theta1_stock_t1_splag",
                "lr_topic_ste_theta2_stock_t1_splag",
                "lr_topic_ste_theta3_stock_t1_splag",
                "lr_topic_ste_theta4_stock_t1_splag",
                "lr_topic_ste_theta5_stock_t1_splag",
                "lr_topic_ste_theta6_stock_t1_splag",
                "lr_topic_ste_theta7_stock_t1_splag",
                "lr_topic_ste_theta8_stock_t1_splag",
                "lr_topic_ste_theta9_stock_t1_splag",
                "lr_topic_ste_theta10_stock_t1_splag",
                "lr_topic_ste_theta11_stock_t1_splag",
                "lr_topic_ste_theta12_stock_t1_splag",
                "lr_topic_ste_theta13_stock_t1_splag",
                "lr_topic_ste_theta14_stock_t1_splag",
            ],
            "RobustScaler->MinMaxScaler": [
                "lr_topic_tokens_t1",
                "lr_topic_tokens_t1_splag",
            ],
            "SqrtTransform->MinMaxScaler": [
                "lr_wdi_sp_dyn_imrt_fe_in",
            ],
            "AsinhTransform->MinMaxScaler": [
                "lr_ged_sb",
                "lr_ged_ns",
                "lr_ged_os",
                "lr_acled_sb",
                "lr_acled_sb_count",
                "lr_acled_os",
                "lr_ged_sb_tsum_24",
                "lr_splag_1_ged_sb",
                "lr_splag_1_ged_os",
                "lr_splag_1_ged_ns",
                "lr_wdi_ny_gdp_mktp_kd",
                "lr_wdi_nv_agr_totl_kn",
                "lr_wdi_sm_pop_netm",
                "lr_wdi_sm_pop_refg_or",
            ],
            "StandardScaler->MinMaxScaler": [
                "lr_wdi_sp_pop_grow",
            ],
        },
        "force_reset": True,
        "gradient_clip_val": 0.3835325210772753,
        "input_chunk_length": 48,
        "loss_function": "WeightedPenaltyHuberLoss",
        "lr": 0.00001962085177538797,
        "lr_scheduler_factor": 0.3845231101837119,
        "lr_scheduler_min_lr": 1e-7,
        "lr_scheduler_patience": 15,
        "n_epochs": 300,
        "nhead": 4,
        "non_zero_weight": 2.4392547205293247,
        "norm_type": "RMSNorm",
        "num_decoder_layers": 2,
        "num_encoder_layers": 2,
        "output_chunk_shift": 0,
        "target_scaler": "AsinhTransform->MinMaxScaler",
        "use_reversible_instance_norm": True,
        "weight_decay": 0.0004461786524443319,
        "zero_threshold": 0.0355543085430714,
    }


    return hyperparameters