{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyro_mcmc_sampler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyroMCMCSampler\n",
      "File \u001b[0;32m~/Documents/scripts/views_platform/views-models/models/purple_alien/notebooks/pyro_mcmc_sampler.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyro\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MCMC, NUTS\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyro'"
     ]
    }
   ],
   "source": [
    "from pyro_mcmc_sampler import PyroMCMCSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = MCMCSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcmc_sampler:\n",
      "🧪 Running Tests on MCMC Sampler with Empirical Samples...\n",
      "\n",
      "INFO:mcmc_sampler:🔹 Running Test: Normal Distribution Samples\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: False\n",
      "INFO:mcmc_sampler:   ✅ Completed in 4.416 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 0.056927\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 3.984919\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [0.000000, 0.549489]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [0.000000, 1.990308]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [0.000000, 2.837897]\n",
      "INFO:mcmc_sampler:🔹 Running Test: Poisson Count Data\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: True\n",
      "INFO:mcmc_sampler:   ✅ Completed in 3.540 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 3.939578\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 12.291484\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [3.463201, 6.167274]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [0.806401, 9.033045]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [0.000000, 10.511351]\n",
      "INFO:mcmc_sampler:   🔍 ✅ All values are non-negative (constraint satisfied)\n",
      "INFO:mcmc_sampler:🔹 Running Test: Gamma Distributed Samples\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: True\n",
      "INFO:mcmc_sampler:   ✅ Completed in 2.872 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 1.867445\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 14.939561\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [0.935328, 5.044742]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [0.000000, 10.573193]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [0.000000, 12.570147]\n",
      "INFO:mcmc_sampler:   🔍 ✅ All values are non-negative (constraint satisfied)\n",
      "INFO:mcmc_sampler:🔹 Running Test: Extreme Right-Skewed Data\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: True\n",
      "INFO:mcmc_sampler:   ✅ Completed in 1.992 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 0.118872\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 7.370071\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [0.000000, 1.324673]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [0.000000, 4.590926]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [0.000000, 5.779553]\n",
      "INFO:mcmc_sampler:   🔍 ✅ All values are non-negative (constraint satisfied)\n",
      "INFO:mcmc_sampler:🔹 Running Test: Heavy-Tailed Student's t\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: False\n",
      "INFO:mcmc_sampler:   ✅ Completed in 3.031 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 0.079695\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 7.331909\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [0.000000, 0.749657]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [0.000000, 3.490173]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [0.000000, 5.302217]\n",
      "INFO:mcmc_sampler:🔹 Running Test: Sparse Data (90% Zero)\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: True\n",
      "INFO:mcmc_sampler:   ✅ Completed in 2.937 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 9.582063\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 16.049868\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [8.429372, 11.163249]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [6.090442, 13.739885]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [5.414096, 15.174634]\n",
      "INFO:mcmc_sampler:   🔍 ✅ All values are non-negative (constraint satisfied)\n",
      "INFO:mcmc_sampler:🔹 Running Test: Floating-Point Precision Test\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: False\n",
      "INFO:mcmc_sampler:   ✅ Completed in 15.443 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 0.000010\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 0.000010\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [0.000010, 0.000010]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [0.000010, 0.000010]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [0.000010, 0.000010]\n",
      "INFO:mcmc_sampler:🔹 Running Test: Extreme Large Values\n",
      "INFO:mcmc_sampler:📊 Sample distribution set with 10000 points.\n",
      "INFO:mcmc_sampler:   📊 Sample Distribution Set with 10000 points\n",
      "INFO:mcmc_sampler:   🔧 Force Non-Negative: False\n",
      "INFO:mcmc_sampler:   ✅ Completed in 4.395 sec\n",
      "INFO:mcmc_sampler:   🔍 MAP Estimate: 999994.460278\n",
      "INFO:mcmc_sampler:   🔍 Max Value: 1000014.232723\n",
      "INFO:mcmc_sampler:   🔍 HDI 50%: [999992.314913, 1000005.848147]\n",
      "INFO:mcmc_sampler:   🔍 HDI 95%: [999977.989528, 1000013.616116]\n",
      "INFO:mcmc_sampler:   🔍 HDI 99%: [999970.398609, 1000012.918724]\n",
      "INFO:mcmc_sampler:\n",
      "✅ All MCMC Sampler Tests Passed Successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler.run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Tuple, Any\n",
    "import math\n",
    "import time\n",
    "\n",
    "class MCMCSampler:\n",
    "    def __init__(self, log_posterior: Callable[[float], float], proposal_std: float = 0.5, \n",
    "                 force_non_negative: bool = True, random_seed: int = None):\n",
    "        \"\"\"\n",
    "        MCMC Sampler with Metropolis-Hastings algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - log_posterior: Callable function that computes the log posterior probability.\n",
    "        - proposal_std: Standard deviation for the proposal distribution.\n",
    "        - force_non_negative: If True, ensures all samples are >= 0 (useful for count data).\n",
    "        - random_seed: Optional seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)  # Class-specific logger\n",
    "        logging.basicConfig(level=logging.INFO)  # Configure logging format\n",
    "        #self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.log_posterior = log_posterior\n",
    "        self.proposal_std = proposal_std\n",
    "        self.force_non_negative = force_non_negative\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "    \n",
    "    def metropolis_hastings(self, initial_value: float, num_samples: int = 10000, \n",
    "                            burn_in: int = 1000, thin: int = 1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Runs the Metropolis-Hastings MCMC algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_value: Starting point for the chain.\n",
    "        - num_samples: Number of samples to collect (after burn-in).\n",
    "        - burn_in: Number of initial samples to discard.\n",
    "        - thin: Keeps every `thin`-th sample to reduce autocorrelation.\n",
    "\n",
    "        Returns:\n",
    "        - Array of sampled values.\n",
    "        \"\"\"\n",
    "        samples = np.empty(num_samples + burn_in)\n",
    "        samples[0] = max(0, initial_value) if self.force_non_negative else initial_value\n",
    "        current = samples[0]\n",
    "        current_log_post = self.log_posterior(current)\n",
    "\n",
    "        for i in range(1, num_samples + burn_in):\n",
    "            proposal = np.random.normal(current, self.proposal_std)\n",
    "            if self.force_non_negative:\n",
    "                proposal = max(0, proposal)  # Ensures only non-negative proposals if required\n",
    "            \n",
    "            proposal_log_post = self.log_posterior(proposal)\n",
    "            log_acceptance_ratio = proposal_log_post - current_log_post\n",
    "\n",
    "            if np.log(np.random.rand()) < log_acceptance_ratio:\n",
    "                current = proposal\n",
    "                current_log_post = proposal_log_post\n",
    "\n",
    "            samples[i] = current\n",
    "\n",
    "        return np.maximum(samples[burn_in:][::thin], 0) if self.force_non_negative else samples[burn_in:][::thin]\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_hdi(samples: np.ndarray, credible_interval: float = 0.95) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Computes the Highest Density Interval (HDI).\n",
    "\n",
    "        Parameters:\n",
    "        - samples: The MCMC samples.\n",
    "        - credible_interval: The probability mass contained in the HDI.\n",
    "\n",
    "        Returns:\n",
    "        - Lower and upper bounds of the HDI.\n",
    "        \"\"\"\n",
    "        samples = np.sort(samples)\n",
    "        n = len(samples)\n",
    "        ci_index = int(np.floor(credible_interval * n))\n",
    "        intervals = np.array([samples[i + ci_index] - samples[i] for i in range(n - ci_index)])\n",
    "        min_index = np.argmin(intervals)\n",
    "        return float(samples[min_index]), float(samples[min_index + ci_index])\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_map(samples: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the Maximum A Posteriori (MAP) estimate.\n",
    "\n",
    "        Parameters:\n",
    "        - samples: The MCMC samples.\n",
    "\n",
    "        Returns:\n",
    "        - MAP estimate (mode of the posterior).\n",
    "        \"\"\"\n",
    "        hist, bin_edges = np.histogram(samples, bins=\"auto\", density=True)  # Adaptive bins\n",
    "        max_bin_index = np.argmax(hist)\n",
    "        return (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2  \n",
    "\n",
    "    def compute_summary(self, samples: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Computes MAP, Max value, and HDI intervals in a structured format.\n",
    "\n",
    "        Parameters:\n",
    "        - samples: The MCMC samples.\n",
    "\n",
    "        Returns:\n",
    "        - Dictionary containing structured results for DataFrame conversion.\n",
    "        \"\"\"\n",
    "        map_estimate = self.compute_map(samples)\n",
    "        max_value = float(np.max(samples))\n",
    "\n",
    "        hdi_50 = self.compute_hdi(samples, 0.50)\n",
    "        hdi_95 = self.compute_hdi(samples, 0.95)\n",
    "        hdi_99 = self.compute_hdi(samples, 0.99)\n",
    "\n",
    "        return {\n",
    "            \"map\": map_estimate,\n",
    "            \"max\": max_value,\n",
    "            \"hdi50lb\": hdi_50[0], \"hdi50ub\": hdi_50[1],\n",
    "            \"hdi95lb\": hdi_95[0], \"hdi95ub\": hdi_95[1],\n",
    "            \"hdi99lb\": hdi_99[0], \"hdi99ub\": hdi_99[1]\n",
    "        }\n",
    "    \n",
    "    def run_tests(self):\n",
    "        \"\"\"\n",
    "        Runs a battery of tests to validate correctness, efficiency, and robustness of MCMC sampling.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"\\n🧪 Running Tests on MCMC Sampler...\\n\")\n",
    "\n",
    "        test_cases = [\n",
    "            {\"name\": \"Basic Normal Distribution\", \"log_posterior\": lambda x: -0.5 * x**2, \"initial_value\": 0.0, \"force_non_negative\": False},\n",
    "            {\"name\": \"Poisson Count Data\", \"log_posterior\": lambda x: -np.inf if x < 0 else x * np.log(5) - 5 - math.lgamma(x + 1), \"initial_value\": 5, \"force_non_negative\": True},\n",
    "            {\"name\": \"Extreme Peak Distribution\", \"log_posterior\": lambda x: -1e6 * (x - 100) ** 2, \"initial_value\": 100, \"force_non_negative\": False},\n",
    "            {\"name\": \"Wide Proposal Step\", \"log_posterior\": lambda x: -0.5 * x**2, \"initial_value\": 0.0, \"force_non_negative\": False},\n",
    "            {\"name\": \"Small Proposal Step\", \"log_posterior\": lambda x: -0.5 * x**2, \"initial_value\": 0.0, \"force_non_negative\": False},\n",
    "            {\"name\": \"Floating-Point Precision Test\", \"log_posterior\": lambda x: -0.5 * (x / 1e-5) ** 2, \"initial_value\": 1e-5, \"force_non_negative\": False},\n",
    "            {\"name\": \"Extreme Negative Log Posterior\", \"log_posterior\": lambda x: -1e8 * x**2, \"initial_value\": 10, \"force_non_negative\": False},\n",
    "        ]\n",
    "\n",
    "        for test in test_cases:\n",
    "            self.logger.info(f\"🔹 Running Test: {test['name']}\")\n",
    "\n",
    "            sampler = MCMCSampler(\n",
    "                log_posterior=test[\"log_posterior\"], \n",
    "                proposal_std=1.0 if \"Wide\" not in test[\"name\"] else 5.0,\n",
    "                force_non_negative=test[\"force_non_negative\"]\n",
    "            )\n",
    "\n",
    "            # Run sampling\n",
    "            start_time = time.time()\n",
    "            samples = sampler.metropolis_hastings(initial_value=test[\"initial_value\"], num_samples=5000)\n",
    "            end_time = time.time()\n",
    "\n",
    "            summary = sampler.compute_summary(samples)\n",
    "\n",
    "            self.logger.info(f\"   ✅ Completed in {end_time - start_time:.3f} sec\")\n",
    "            self.logger.info(f\"   🔍 MAP Estimate: {summary['map']:.6f}\")\n",
    "            self.logger.info(f\"   🔍 Max Value: {summary['max']:.6f}\")\n",
    "            self.logger.info(f\"   🔍 HDI 50%: [{summary['hdi50lb']:.6f}, {summary['hdi50ub']:.6f}]\")\n",
    "            self.logger.info(f\"   🔍 HDI 95%: [{summary['hdi95lb']:.6f}, {summary['hdi95ub']:.6f}]\")\n",
    "            self.logger.info(f\"   🔍 HDI 99%: [{summary['hdi99lb']:.6f}, {summary['hdi99ub']:.6f}]\")\n",
    "\n",
    "            # **Validation Checks**\n",
    "            assert summary[\"map\"] is not None, \"❌ MAP estimate is missing!\"\n",
    "            assert summary[\"max\"] >= min(samples), \"❌ Max value incorrect!\"\n",
    "            if test[\"force_non_negative\"]:\n",
    "                assert np.all(samples >= 0), \"❌ Negative values found in non-negative sampling!\"\n",
    "\n",
    "        self.logger.info(\"\\n✅ All MCMC Sampler Tests Passed Successfully!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ **Define an Example Log-Posterior Function**\n",
    "def normal_log_posterior(x):\n",
    "    \"\"\"Example: Standard normal log posterior.\"\"\"\n",
    "    return -0.5 * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sampler = MCMCSampler(log_posterior=normal_log_posterior, proposal_std=1.0, force_non_negative=False)\n",
    "    \n",
    "    # ✅ **Run Automated Tests**\n",
    "    sampler.run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd  # Required for DataFrame integration\n",
    "\n",
    "    sampler = MCMCSampler(log_posterior, proposal_std=1.0, force_non_negative=True, random_seed=42)\n",
    "\n",
    "    # Simulating multiple distributions\n",
    "    results = []\n",
    "    for initial_value in [3, 5, 10]:  # Different starting points\n",
    "        samples = sampler.metropolis_hastings(initial_value=initial_value, num_samples=10000)\n",
    "        summary = sampler.compute_summary(samples)\n",
    "        results.append(summary)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "from typing import Callable, List, Dict, Tuple\n",
    "\n",
    "class MCMCSampler:\n",
    "    def __init__(self, log_posterior: Callable[[float], float], proposal_std: float = 0.5, random_seed: int = None):\n",
    "        self.log_posterior = log_posterior\n",
    "        self.proposal_std = proposal_std\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "    \n",
    "    def metropolis_hastings(self, initial_value: float, num_samples: int = 10000, burn_in: int = 1000, thin: int = 1) -> np.ndarray:\n",
    "        samples = np.empty(num_samples + burn_in)\n",
    "        samples[0] = max(0, initial_value)  # Ensure initial sample is non-negative\n",
    "        current = samples[0]\n",
    "        current_log_post = self.log_posterior(current)\n",
    "\n",
    "        for i in range(1, num_samples + burn_in):\n",
    "            # Ensure non-negative proposals\n",
    "            proposal = max(0, np.random.normal(current, self.proposal_std))  \n",
    "            proposal_log_post = self.log_posterior(proposal)\n",
    "            log_acceptance_ratio = proposal_log_post - current_log_post\n",
    "\n",
    "            if np.log(np.random.rand()) < log_acceptance_ratio:\n",
    "                current = proposal\n",
    "                current_log_post = proposal_log_post\n",
    "\n",
    "            samples[i] = current\n",
    "\n",
    "        return np.maximum(samples[burn_in:][::thin], 0)  # Ensure final outputs are non-negative\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_hdi(samples: np.ndarray, credible_interval: float = 0.95) -> Tuple[int, int]:\n",
    "        samples = np.sort(samples)\n",
    "        n = len(samples)\n",
    "        ci_index = int(np.floor(credible_interval * n))\n",
    "        intervals = np.array([samples[i + ci_index] - samples[i] for i in range(n - ci_index)])\n",
    "        min_index = np.argmin(intervals)\n",
    "        return int(round(samples[min_index])), int(round(samples[min_index + ci_index]))  # Ensure integer output\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_map(samples: np.ndarray) -> int:\n",
    "        hist, bin_edges = np.histogram(samples, bins=np.arange(0, np.max(samples) + 1), density=True)  # Integer bins\n",
    "        max_bin_index = np.argmax(hist)\n",
    "        return int(bin_edges[max_bin_index])  # Return integer MAP\n",
    "\n",
    "    def compute_summary(self, samples: np.ndarray, credible_intervals: List[float] = [0.5, 0.75, 0.95]) -> Dict[str, any]:\n",
    "        map_estimate = self.compute_map(samples)\n",
    "        hdi_intervals = {ci: self.compute_hdi(samples, ci) for ci in credible_intervals}\n",
    "        max_value = int(np.max(samples))\n",
    "\n",
    "        return {\n",
    "            \"MAP\": map_estimate,\n",
    "            \"HDIs\": hdi_intervals,\n",
    "            \"Max Value\": max_value\n",
    "        }\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    def log_posterior(x):\n",
    "        \"\"\"Example: Poisson log-posterior (approx) for count data\"\"\"\n",
    "        lambda_param = 5  # Mean count rate\n",
    "        if x < 0:\n",
    "            return -np.inf  # Log-prob of negative counts is undefined\n",
    "        return x * np.log(lambda_param) - lambda_param - math.lgamma(x + 1)\n",
    "\n",
    "    sampler = MCMCSampler(log_posterior, proposal_std=1.0, random_seed=42)\n",
    "    samples = sampler.metropolis_hastings(initial_value=5, num_samples=10000)\n",
    "    summary = sampler.compute_summary(samples)\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMCSampler:\n",
    "    def __init__(self, log_posterior, proposal_std=0.5):\n",
    "        self.log_posterior = log_posterior\n",
    "        self.proposal_std = proposal_std\n",
    "\n",
    "    def metropolis_hastings(self, initial_value, num_samples=10000, burn_in=1000, thin=1):\n",
    "        samples = np.empty(num_samples + burn_in)\n",
    "        samples[0] = initial_value\n",
    "        current = initial_value\n",
    "        current_log_post = self.log_posterior(current)\n",
    "\n",
    "        for i in range(1, num_samples + burn_in):\n",
    "            proposal = np.abs(np.random.normal(current, self.proposal_std))  # Keep values positive\n",
    "            proposal_log_post = self.log_posterior(proposal)\n",
    "            log_acceptance_ratio = proposal_log_post - current_log_post\n",
    "\n",
    "            if np.log(np.random.rand()) < log_acceptance_ratio:\n",
    "                current = proposal\n",
    "                current_log_post = proposal_log_post\n",
    "\n",
    "            samples[i] = current\n",
    "\n",
    "        return samples[burn_in:][::thin]  # Discard burn-in and thin chain\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_hdi(samples, credible_interval=0.95):\n",
    "        samples = np.sort(np.asarray(samples))\n",
    "        ci_index = max(1, int(np.floor(credible_interval * len(samples))))\n",
    "\n",
    "        intervals_min = samples[:len(samples) - ci_index]\n",
    "        intervals_max = samples[ci_index:]\n",
    "        interval_widths = intervals_max - intervals_min\n",
    "\n",
    "        min_index = np.argmin(interval_widths)\n",
    "        return intervals_min[min_index], intervals_max[min_index]\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_map(samples):\n",
    "        \"\"\"Compute the Maximum A Posteriori (MAP) estimate (highest density point).\"\"\"\n",
    "        hist, bin_edges = np.histogram(samples, bins=50, density=True)  # Histogram density\n",
    "        max_bin_index = np.argmax(hist)  # Find bin with max density\n",
    "        map_estimate = (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2  # Midpoint of bin\n",
    "        return map_estimate\n",
    "\n",
    "    def compute_summary(self, samples, credible_intervals=[0.5, 0.75, 0.95]):\n",
    "        \"\"\"Compute MAP, HDIs, and max value.\"\"\"\n",
    "        map_estimate = self.compute_map(samples)\n",
    "        hdi_intervals = {ci: self.compute_hdi(samples, ci) for ci in credible_intervals}\n",
    "        max_value = np.max(samples)\n",
    "\n",
    "        return {\n",
    "            \"MAP\": map_estimate,\n",
    "            \"HDIs\": hdi_intervals,\n",
    "            \"Max Value\": max_value\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Tuple\n",
    "\n",
    "class MCMCSampler:\n",
    "    def __init__(self, log_posterior: Callable[[float], float], proposal_std: float = 0.5, random_seed: int = None):\n",
    "        self.log_posterior = log_posterior\n",
    "        self.proposal_std = proposal_std\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "    \n",
    "    def metropolis_hastings(self, initial_value: float, num_samples: int = 10000, burn_in: int = 1000, thin: int = 1) -> np.ndarray:\n",
    "        samples = np.empty(num_samples + burn_in)\n",
    "        samples[0] = initial_value\n",
    "        current = initial_value\n",
    "        current_log_post = self.log_posterior(current)\n",
    "\n",
    "        for i in range(1, num_samples + burn_in):\n",
    "            proposal = np.random.normal(current, self.proposal_std)  # Removed np.abs() to avoid bias\n",
    "            proposal_log_post = self.log_posterior(proposal)\n",
    "            log_acceptance_ratio = proposal_log_post - current_log_post\n",
    "\n",
    "            if np.log(np.random.rand()) < log_acceptance_ratio:\n",
    "                current = proposal\n",
    "                current_log_post = proposal_log_post\n",
    "\n",
    "            samples[i] = current\n",
    "\n",
    "        return samples[burn_in:][::thin]  \n",
    "\n",
    "    @staticmethod\n",
    "    def compute_hdi(samples: np.ndarray, credible_interval: float = 0.95) -> Tuple[float, float]:\n",
    "        samples = np.sort(samples)\n",
    "        n = len(samples)\n",
    "        ci_index = int(np.floor(credible_interval * n))\n",
    "        intervals = np.array([samples[i + ci_index] - samples[i] for i in range(n - ci_index)])\n",
    "        min_index = np.argmin(intervals)\n",
    "        return samples[min_index], samples[min_index + ci_index]\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_map(samples: np.ndarray) -> float:\n",
    "        hist, bin_edges = np.histogram(samples, bins=\"auto\", density=True)  # Adaptive bins\n",
    "        max_bin_index = np.argmax(hist)\n",
    "        return (bin_edges[max_bin_index] + bin_edges[max_bin_index + 1]) / 2  \n",
    "\n",
    "    def compute_summary(self, samples: np.ndarray, credible_intervals: List[float] = [0.5, 0.75, 0.95]) -> Dict[str, any]:\n",
    "        map_estimate = self.compute_map(samples)\n",
    "        hdi_intervals = {ci: self.compute_hdi(samples, ci) for ci in credible_intervals}\n",
    "        max_value = np.max(samples)\n",
    "\n",
    "        return {\n",
    "            \"MAP\": map_estimate,\n",
    "            \"HDIs\": hdi_intervals,\n",
    "            \"Max Value\": max_value\n",
    "        }\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    def log_posterior(x):\n",
    "        return -0.5 * (x ** 2)  # Example: standard normal log-posterior\n",
    "\n",
    "    sampler = MCMCSampler(log_posterior, proposal_std=1.0, random_seed=42)\n",
    "    samples = sampler.metropolis_hastings(initial_value=0.0, num_samples=10000)\n",
    "    summary = sampler.compute_summary(samples)\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Zero-Inflated Right-Skewed Log-Posterior\n",
    "def log_posterior(theta):\n",
    "    p = 0.3  # Probability of zero inflation\n",
    "    alpha, beta = 2, 1  # Gamma distribution shape and scale\n",
    "\n",
    "    data = np.array([0, 0, 0, 1.2, 0.5, 3.0, 7.5, 2.3, 8.1, 0])  # Example data with zeros\n",
    "    log_likelihood = np.sum(\n",
    "        np.log(p * (data == 0) + (1 - p) * stats.gamma.pdf(data, a=alpha, scale=1/beta) * (data > 0))\n",
    "    )\n",
    "\n",
    "    log_prior = stats.gamma.logpdf(theta, a=alpha, scale=1/beta)  # Prior for theta\n",
    "    return log_likelihood + log_prior  # Posterior ∝ Likelihood * Prior\n",
    "\n",
    "# Run MCMC Sampling\n",
    "sampler = MCMCSampler(log_posterior, proposal_std=0.5)\n",
    "samples = sampler.metropolis_hastings(initial_value=1.0, num_samples=20000)\n",
    "\n",
    "# Compute Summary Stats\n",
    "summary = sampler.compute_summary(samples, credible_intervals=[0.5, 0.75, 0.95])\n",
    "\n",
    "# Extract values\n",
    "map_estimate = summary[\"MAP\"]\n",
    "hdi_intervals = summary[\"HDIs\"]\n",
    "max_value = summary[\"Max Value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_posterior(samples, hdi_intervals, map_estimate, max_value, color_scheme=\"gray\"):\n",
    "    \"\"\"\n",
    "    Plots a posterior distribution with HDIs, MAP estimate, and max sampled value\n",
    "    using a clean, scientific, and Tufte-inspired aesthetic.\n",
    "\n",
    "    Args:\n",
    "        samples (array-like): Posterior samples.\n",
    "        hdi_intervals (dict): Dictionary of HDI intervals {credible_level: (low, high)}.\n",
    "        map_estimate (float): Maximum a posteriori (MAP) estimate.\n",
    "        max_value (float): Maximum sampled value.\n",
    "        color_scheme (str): \"gray\" for shades of gray, \"blue\" for shades of blue.\n",
    "    \"\"\"\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Choose colors based on the scheme\n",
    "    if color_scheme == \"gray\":\n",
    "        hdi_colors = [\"#D9D9D9\", \"#A6A6A6\", \"#737373\"]  # Light, Medium, Dark Gray\n",
    "    elif color_scheme == \"blue\":\n",
    "        hdi_colors = [\"#C6DBEF\", \"#6BAED6\", \"#2171B5\"]  # Light, Medium, Dark Blue\n",
    "    else:\n",
    "        raise ValueError(\"Invalid color_scheme. Use 'gray' or 'blue'.\")\n",
    "\n",
    "    # Plot smooth density (KDE) instead of histogram\n",
    "    sns.kdeplot(samples, color=\"black\", linewidth=1.5, fill=True, alpha=0.2, label=\"Posterior Density\")\n",
    "\n",
    "    # HDI shading for multiple credible intervals (largest first)\n",
    "    for idx, (ci, (hdi_low, hdi_high)) in enumerate(sorted(hdi_intervals.items(), reverse=True)):\n",
    "        label = f\"{int(ci*100)}% HDI\"\n",
    "        color = hdi_colors[idx % len(hdi_colors)]  # Cycle through the three colors\n",
    "        ax.axvspan(hdi_low, hdi_high, color=color, alpha=0.5, label=label)\n",
    "\n",
    "    # MAP Estimate (Highest Density Point)\n",
    "    ax.axvline(map_estimate, color=\"black\", linestyle=\"-\", linewidth=1.5, label=\"MAP Estimate\")\n",
    "    ax.text(map_estimate, plt.ylim()[1] * 0.05, \"MAP\", ha=\"center\", fontsize=11, color=\"black\")\n",
    "\n",
    "    # Max Value (Extreme Sample)\n",
    "    ax.axvline(max_value, color=\"red\", linestyle=\":\", linewidth=1.2, label=\"Max Sampled Value\")\n",
    "    ax.text(max_value, plt.ylim()[1] * 0.07, \"Max\", ha=\"left\", fontsize=11, color=\"red\")\n",
    "\n",
    "    # Minimalist aesthetics\n",
    "    sns.despine()  # Remove spines for cleaner look\n",
    "    ax.set_yticks([])  # Remove y-axis ticks\n",
    "    ax.set_xlabel(r\"Predicted Fatalities (ln)\", fontsize=14)  # LaTeX-style label\n",
    "    ax.set_ylabel(\"\")  # No y-label for cleaner aesthetics\n",
    "\n",
    "    # Add legend with all HDIs, MAP, and Max\n",
    "    ax.legend(loc=\"upper right\", frameon=False, fontsize=11)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage:\n",
    "plot_posterior(samples, hdi_intervals, map_estimate, max_value, color_scheme=\"blue\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
